{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6b8bba-bcc2-4520-b283-68061e2cdd94",
   "metadata": {
    "id": "0d6b8bba-bcc2-4520-b283-68061e2cdd94"
   },
   "source": [
    "# Tic Tac Toe\n",
    "With Neural Network for State Value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b125a8c-23df-4ade-b33e-067a24fdd36b",
   "metadata": {
    "id": "5b125a8c-23df-4ade-b33e-067a24fdd36b"
   },
   "source": [
    "### Outline of approach:\n",
    "1. Pretrain: Play atleast 100 games and get the training data for state and values as list.  \\\n",
    "    Train a 2 value predictor network on this data - one for P1 and another for P2  \n",
    "2. RL Train: In a loop \\\n",
    "    a) play n games using the trained networks and epsilon greedy approach \\\n",
    "    b) record the outcomes and compute state values \\\n",
    "    c) use this data to retrain the two networks \\\n",
    "3. Train till convergence\n",
    "\n",
    "Reference url for Tic Tac Toe environment: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/TicTacToe/ticTacToe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b46ea6-803e-4b9f-97cf-ac20b66a71b8",
   "metadata": {
    "id": "98b46ea6-803e-4b9f-97cf-ac20b66a71b8"
   },
   "source": [
    "### Basic package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271416f6",
   "metadata": {
    "id": "271416f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "kt5MRKYbMTKy",
   "metadata": {
    "id": "kt5MRKYbMTKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\tumpa_anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69245fc0-915e-40db-a1fc-945136814a7b",
   "metadata": {
    "id": "69245fc0-915e-40db-a1fc-945136814a7b"
   },
   "source": [
    "### Classes and Keras Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "okuxwJeLq9-J",
   "metadata": {
    "id": "okuxwJeLq9-J"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613774a8",
   "metadata": {
    "id": "613774a8"
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    '''\n",
    "    Definition of a Tic-Tac-Toe board\n",
    "    '''\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "\n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "\n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(-1)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(-1)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(0)\n",
    "\n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    def play(self, rounds=2000,inner_rounds=100):\n",
    "        winlist = []\n",
    "        for i in range(rounds):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            if i % 100 == 0:\n",
    "                self.p1.setEps(rounds, i)\n",
    "                self.p2.setEps(rounds, i)\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "\n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "            if i % inner_rounds==0:\n",
    "                self.p1.sVNNtrain()\n",
    "                self.p2.sVNNtrain()\n",
    "            self.reset()\n",
    "            winlist.append(win)\n",
    "        return (winlist)\n",
    "    # play with human\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            #self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    self.reset()\n",
    "                    return 1\n",
    "                else:\n",
    "                    self.reset()\n",
    "                    return 0\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                #self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        self.reset()\n",
    "                        return -1\n",
    "\n",
    "                    else:\n",
    "                        self.reset()\n",
    "                        return 0\n",
    "        return 0\n",
    "    def NNPlay(self,rounds=2000, inner_rounds=100,name_exp= None):\n",
    "        '''\n",
    "        train inner rounds, capture replay buffer and train the s value networks on this data\n",
    "        after every inner round the replay buffer is emptied and the process repeated again\n",
    "        '''\n",
    "\n",
    "        print(\"training...\")\n",
    "        w1=self.play(rounds)\n",
    "\n",
    "        self.p1.state_value_model.save(f\"p1model_{name_exp}.keras\")\n",
    "        self.p2.state_value_model.save(f\"p2model_{name_exp}.keras\")\n",
    "\n",
    "        analysis = []\n",
    "        train_batches = int(rounds/inner_rounds) + 1\n",
    "        for i in range(train_batches):\n",
    "            start = inner_rounds * i\n",
    "            end = inner_rounds * (i + 1)\n",
    "            temp_p = w1[start:end]\n",
    "            p1_wins = temp_p.count(1)\n",
    "            p2_wins = temp_p.count(-1)\n",
    "            ties = temp_p.count(0)\n",
    "            analysis.append([i, start, end, p1_wins, p2_wins, ties])\n",
    "\n",
    "        andf= pd.DataFrame(analysis)\n",
    "        andf.columns = ['batch', 'start', 'end', 'p1win', 'p2win', 'tie']\n",
    "        andf['p1win'] = andf['p1win'].apply(lambda x: x/inner_rounds)\n",
    "        andf['p2win'] = andf['p2win'].apply(lambda x: x/inner_rounds)\n",
    "        andf['tie'] = andf['tie'].apply(lambda x: x/inner_rounds)\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(andf['batch'], andf['p1win'])\n",
    "        plt.plot(andf['batch'], andf['p2win'])\n",
    "        plt.plot(andf['batch'], andf['tie'])\n",
    "        plt.legend(['p1 win', 'p2 win', 'tie'])\n",
    "        #plt.title(title_string)\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d1328e",
   "metadata": {
    "id": "c5d1328e"
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Class for one Tic Tac Toe player\n",
    "    '''\n",
    "    def __init__(self, name, eps_decay=False, start_exp_rate=0.3, end_exp_rate=0.05):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 1.0\n",
    "        self.exp_rate = start_exp_rate\n",
    "\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "\n",
    "        self.eps_decay = eps_decay\n",
    "        self.start_exp_rate = start_exp_rate\n",
    "        self.end_exp_rate = end_exp_rate\n",
    "        self.state_value_model = self.sValueNN()\n",
    "\n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return boardHash\n",
    "\n",
    "    def getSVal(self, board):\n",
    "        bs = board.reshape(BOARD_COLS * BOARD_ROWS)\n",
    "        bost = np.reshape(bs, (-1, 9))\n",
    "        sval = self.state_value_model.predict(bost, verbose=False)[0][0]\n",
    "        return sval\n",
    "\n",
    "    def sValueNN(self):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(Dense(units=4, input_dim=9, activation='linear'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "        return (model)\n",
    "\n",
    "    def getbuffer(self):\n",
    "        data = self.states_value\n",
    "        ll = []\n",
    "        for k in data.keys():\n",
    "            yy = re.findall(r'[-/+]?\\d+\\.*\\d*', k)\n",
    "            zz = data.get(k)\n",
    "            yy.append(zz)\n",
    "            ll.append(yy)\n",
    "        lldf = pd.DataFrame(ll)\n",
    "        cols = ['x' + str(i) for i in range(9)]\n",
    "        cols.append('val')\n",
    "        lldf.columns = cols\n",
    "        lldf.to_csv('Buffer.csv', index=False)\n",
    "        return\n",
    "\n",
    "\n",
    "    def sVNNtrain(self, Xin = None, Yin = None):\n",
    "        # call this function to create buffer\n",
    "        self.getbuffer()\n",
    "        # read the buffer\n",
    "        df = pd.read_csv('Buffer.csv')\n",
    "        print ('length of buffer is', len(df))\n",
    "        traincols=['x' + str(i) for i in range(9)]\n",
    "        testcol = 'val'\n",
    "        if Xin is None:\n",
    "            Xin = df[traincols]\n",
    "            Yin = df[testcol]\n",
    "        self.state_value_model.fit(Xin, Yin, epochs=10, verbose=False)\n",
    "\n",
    "        # empty the replay buffer after this\n",
    "        self.states_value = {}\n",
    "        return\n",
    "\n",
    "    def loadSVNNmodel(self, path):\n",
    "        self.state_value_model = keras.models.load_model(path)\n",
    "\n",
    "    def setEps(self, total_games, current_game):\n",
    "        if (self.eps_decay == False):\n",
    "            return\n",
    "        else:\n",
    "\n",
    "            self.exp_rate = self.start_exp_rate * (1. - current_game/total_games) + self.end_exp_rate * (current_game/total_games)\n",
    "            if (np.mod(current_game, 100) == 0):\n",
    "                print ('decay rate modified at {} with current value of {}'.format(str(current_game), str(self.exp_rate)))\n",
    "        return\n",
    "\n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # Default method uses a dictionary to get the value of a state\n",
    "                #value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "\n",
    "                # New method use the keras model to predict the state value\n",
    "                value = self.getSVal(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0645a9",
   "metadata": {
    "id": "6b0645a9"
   },
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    '''\n",
    "    Class for a human player\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9896a789",
   "metadata": {
    "id": "9896a789"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def chooseAction(self, positions,board, playerSymbol):\n",
    "        return random.choice(positions) #random selection\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6807b50-30a4-4c93-876c-e03510f69121",
   "metadata": {
    "id": "c6807b50-30a4-4c93-876c-e03510f69121"
   },
   "source": [
    "def definemodel():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=4, input_dim=9, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00d8e9-3f5e-4237-9529-84126d5a4fc6",
   "metadata": {
    "id": "5b00d8e9-3f5e-4237-9529-84126d5a4fc6"
   },
   "source": [
    "Define the NN models for P1 and P2, one time. \\\n",
    "They do not change during the course of training, only the weights get updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572034e0",
   "metadata": {
    "id": "572034e0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e8f8bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d0e8f8bb",
    "outputId": "a09dd412-b46e-4369-8fae-f729c8c40be0"
   },
   "outputs": [],
   "source": [
    "# # Expt 1: Both P1 and P2 follow constant epsilon\n",
    "# # define P1 and P2 and then train\n",
    "# p1 = Player(\"p1\", eps_decay = False, start_exp_rate=0.3)\n",
    "# p2 = Player(\"p2\", eps_decay = False, start_exp_rate=0.3)\n",
    "# st=State(p1,p2)\n",
    "# st.NNPlay(rounds=2000,name_exp=\"CP1_CP2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nPFcmIqvx7qk",
   "metadata": {
    "id": "nPFcmIqvx7qk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69735097",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "69735097",
    "outputId": "75254051-24d9-4735-f543-16d9c4367776"
   },
   "outputs": [],
   "source": [
    "# # define P1 and P2 and then train\n",
    "# p1 = Player(\"p1\", eps_decay = True, start_exp_rate=0.3, end_exp_rate=0.05)\n",
    "# p2 = Player(\"p2\", eps_decay = False, start_exp_rate=0.3)\n",
    "# st=State(p1,p2)\n",
    "# st.NNPlay(rounds=2000,name_exp=\"dP1_CP2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00773da2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "00773da2",
    "outputId": "e50c8a49-8630-4fbb-937b-f2e10b603910"
   },
   "outputs": [],
   "source": [
    "# # define P1 and P2 and then train\n",
    "# p1 = Player(\"p1\", eps_decay = True, start_exp_rate=0.3, end_exp_rate=0.05)\n",
    "# p2 = Player(\"p2\", eps_decay = True, start_exp_rate=0.3,end_exp_rate=0.05)\n",
    "# st=State(p1,p2)\n",
    "# st.NNPlay(rounds=2000,name_exp=\"dP1_dP2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d96d21",
   "metadata": {
    "id": "a0d96d21"
   },
   "outputs": [],
   "source": [
    "# Expt 1: Both P1 and P2 follow constant epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2eeb0c3",
   "metadata": {
    "id": "f2eeb0c3"
   },
   "outputs": [],
   "source": [
    "# Test the learning\n",
    "# Test Method 1: Play against random player\n",
    "\n",
    "def testgame(p1, p2, ngames):\n",
    "    p1_wins = 0\n",
    "    p2_wins = 0\n",
    "    ties = 0\n",
    "\n",
    "    for _ in range(ngames):\n",
    "        state = State(p1, p2)  #Computer (p1) play first against Human (p2) player\n",
    "        win = state.play2()\n",
    "        if win == 1:\n",
    "            p1_wins += 1\n",
    "        elif win == -1:\n",
    "            p2_wins += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "\n",
    "    p1_win_probability = p1_wins / ngames\n",
    "    p2_win_probability = p2_wins / ngames\n",
    "    tie_probability = ties / ngames\n",
    "    return p1_win_probability,p2_win_probability,tie_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "DjtEb8XkxlG6",
   "metadata": {
    "id": "DjtEb8XkxlG6"
   },
   "outputs": [],
   "source": [
    "## First player as computer and Second player as Random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8olndBv0xW7c",
   "metadata": {
    "id": "8olndBv0xW7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\tumpa_anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\tumpa_anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Player 1 (Computer): Winning Probability: 78.20%\n",
      "Player 2 (RandomPlayer): Winning Probability: 15.30%\n",
      "Tie Probability: 6.50%\n"
     ]
    }
   ],
   "source": [
    "# Test 1 - player trained with constant epsilon\n",
    "\n",
    "p1 = Player(\"Computer\", start_exp_rate=0)\n",
    "p1.loadSVNNmodel(\"p1model_dP1_dP2.keras\")\n",
    "p2 = Player(\"RandomPlayer\",start_exp_rate=2)\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vo6cGo47ekbN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "vo6cGo47ekbN",
    "outputId": "ab0a0fe4-8eb1-450e-b4e2-aec6029fe21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (Computer): Winning Probability: 82.20%\n",
      "Player 2 (RandomPlayer): Winning Probability: 9.20%\n",
      "Tie Probability: 8.60%\n"
     ]
    }
   ],
   "source": [
    "# Test 3 - both players trained with decaying epsilon\n",
    "p1 = Player(\"Computer\", start_exp_rate=0)\n",
    "p1.loadSVNNmodel(\"p1model_dP1_CP2.keras\")\n",
    "p2 = Player(\"RandomPlayer\",start_exp_rate=2)\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "D1c_tCzSxWjd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1c_tCzSxWjd",
    "outputId": "40517b3a-7017-40d8-b211-b75d0a93533e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (Computer): Winning Probability: 79.80%\n",
      "Player 2 (RandomPlayer): Winning Probability: 11.80%\n",
      "Tie Probability: 8.40%\n"
     ]
    }
   ],
   "source": [
    "# Test 2 - one player trained with decaying epsilon\n",
    "\n",
    "p1 = Player(\"Computer\", start_exp_rate=0)\n",
    "p1.loadSVNNmodel(\"p1model_DP1_CP2.keras\")\n",
    "p2 = Player(\"RandomPlayer\",start_exp_rate=2)\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yckKeTKHekoI",
   "metadata": {
    "id": "yckKeTKHekoI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5vQXaoiWxXES",
   "metadata": {
    "id": "5vQXaoiWxXES"
   },
   "outputs": [],
   "source": [
    "## First player as Random player and Second player as Computer player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "053c91ad",
   "metadata": {
    "id": "053c91ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (RandomPlayer): Winning Probability: 36.80%\n",
      "Player 2 (Computer): Winning Probability: 52.30%\n",
      "Tie Probability: 10.90%\n"
     ]
    }
   ],
   "source": [
    "# Test 1 - player trained with constant epsilon\n",
    "\n",
    "p1 = Player(\"RandomPlayer\", start_exp_rate=2)\n",
    "p2 = Player(\"Computer\",start_exp_rate=0)\n",
    "p2.loadSVNNmodel(\"p2model_CP1_CP2.keras\")\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d672249",
   "metadata": {
    "id": "5d672249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (RandomPlayer): Winning Probability: 40.10%\n",
      "Player 2 (Computer): Winning Probability: 52.20%\n",
      "Tie Probability: 7.70%\n"
     ]
    }
   ],
   "source": [
    "# Test 2 - one player trained with decaying epsilon\n",
    "\n",
    "p1 = Player(\"RandomPlayer\", start_exp_rate=2)\n",
    "p2 = Player(\"Computer\",start_exp_rate=0)\n",
    "p2.loadSVNNmodel('p2model_dP1_CP2.keras')\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff1aef3",
   "metadata": {
    "id": "fff1aef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (RandomPlayer): Winning Probability: 64.40%\n",
      "Player 2 (Computer): Winning Probability: 26.00%\n",
      "Tie Probability: 9.60%\n"
     ]
    }
   ],
   "source": [
    "# Test 3 - both players trained with decaying epsilon\n",
    "\n",
    "p1 = Player(\"RandomPlayer\",start_exp_rate=2)\n",
    "p2 = Player(\"Computer\",start_exp_rate=0)\n",
    "p2.loadSVNNmodel(\"p2model_dP1_dP2.keras\")\n",
    "\n",
    "p1_win_probability,p2_win_probability,tie_probability=testgame(p1,p2, 1000)\n",
    "print(\"Player 1 ({}): Winning Probability: {:.2%}\".format(p1.name, p1_win_probability))\n",
    "print(\"Player 2 ({}): Winning Probability: {:.2%}\".format(p2.name, p2_win_probability))\n",
    "print(\"Tie Probability: {:.2%}\".format(tie_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6253d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
